{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<head>\n",
    "  <meta name=\"author\" content=\"Rogério de Oliveira\">\n",
    "  <meta institution=\"author\" content=\"Universidade Presbiteriana Mackenzie\">\n",
    "</head>\n",
    "\n",
    "<img src=\"http://meusite.mackenzie.br/rogerio/mackenzie_logo/UPM.2_horizontal_vermelho.jpg\" width=300, align=\"right\" />\n",
    "\n",
    "<h1 align=left><font size=8><b>Inteligência Artificial</b></font></h1>\n",
    "<h1 align=left><font size=6><b>Deep Learning</b></font></h1>\n",
    "\n",
    "# Atividade: T6 - Atividade de aprofundamento\n",
    "\n",
    "Nome: Bruno Rebocho de Toledo\n",
    "\n",
    "Turma: 01B\n",
    "\n",
    "Matrícula: 92316328"
   ],
   "metadata": {
    "id": "vsmo_qbU_k-m"
   },
   "id": "vsmo_qbU_k-m"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Introdução\n",
    "\n",
    "Este notebook tem como objetivo explorar as capacidades de classificação de redes neurais com PyTorch."
   ],
   "metadata": {
    "id": "zWa4QPRf5VHf"
   },
   "id": "zWa4QPRf5VHf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Dataset \"Predict students' dropout and academic success\"[1]\n",
    "\n",
    "O conjunto de dados escolhido, criado a partir de uma instituição de ensino superior (adquirido de várias bases de dados independentes), descreve estudantes matriculados em diferentes graduações, como agronomia, design, educação, enfermagem, jornalismo, gestão, serviço social e tecnologias. O conjunto de dados inclui informações conhecidas no momento da matrícula do estudante (trajetória acadêmica, demografia e fatores socioeconômicos) e o desempenho acadêmico dos alunos ao final do primeiro e segundo semestres. Os dados são usados para construir modelos de classificação para prever a evasão e o sucesso acadêmico."
   ],
   "metadata": {
    "id": "OyDLRklQ5ail"
   },
   "id": "OyDLRklQ5ail"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## O Problema\n",
    "\n",
    "O problema é formulado como uma tarefa de classificação binária indicando a conclusão ou desistência do curso, na qual existe um forte desequilíbrio em favor de uma das classes originais."
   ],
   "metadata": {
    "id": "AZ30CrXR5dT2"
   },
   "id": "AZ30CrXR5dT2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Uso de GPU\n",
    "\n",
    "Iniciamos configurando nosso ambiente de execução para utilizar a GPU, caso disponível. As operações a serem executadas no treinamento deste modelo tendem a serem feitas de maneira muito mais rápida e eficiente em GPUs, devido à natureza da arquitetura deste tipo de chip e as estruturas dos dados aos quais iremos operar."
   ],
   "metadata": {
    "id": "RpViNFwu6rDV"
   },
   "id": "RpViNFwu6rDV"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iXuJgQvyXdu",
    "outputId": "39f6b5c9-fe02-461d-afae-11f3422b5244",
    "ExecuteTime": {
     "end_time": "2023-11-26T00:47:27.552307Z",
     "start_time": "2023-11-26T00:47:27.365339Z"
    }
   },
   "id": "8iXuJgQvyXdu",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      3\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m device\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(device))\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Explorando os Dados\n",
    "\n",
    "Inicialmente, começamos explorando os dados ao importar as bibliotecas `numpy` e `pandas`. Utilizamos o `pandas` para ler o arquivo 'data.csv', especificando o ponto e vírgula como delimitador.\n",
    "\n",
    "Após a leitura do arquivo, exibimos as primeiras cinco linhas do DataFrame `df` com a função `head()`. Esta visualização inicial nos fornece uma visão rápida de como os dados estão estruturados, incluindo nomes de colunas e tipos de dados, o que é crucial para o planejamento das etapas seguintes de análise.\n",
    "\n",
    "Em seguida, imprimimos a dimensão do DataFrame com `df.shape`, revelando o número total de linhas e colunas. Esta informação é fundamental para entender o volume e a complexidade dos dados com os quais estamos trabalhando, orientando as decisões sobre o processamento e a análise subsequente.\n",
    "\n",
    "Por vim, verificamos a presença de nulos ou valores ausentes, o que não é o caso deste nosso conjunto de dados."
   ],
   "metadata": {
    "id": "JfqXdIpx5g1B"
   },
   "id": "JfqXdIpx5g1B"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!wget -O data.csv 'https://drive.google.com/uc?export=download&id=1hnHg7IN3qTPnIui2XrkT5hUd9XFH8Ecn'\n",
    "\n",
    "df = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "print(df.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "MuDRgxra5iBY",
    "outputId": "3efe12dc-6e72-4d6d-fa8a-b0f7f6219190",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.550901Z"
    }
   },
   "id": "MuDRgxra5iBY",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.isnull().sum()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKI0ObxP5mHe",
    "outputId": "4e0874fa-c716-4eba-df08-ed4611ec7764",
    "ExecuteTime": {
     "end_time": "2023-11-26T00:47:27.553877Z",
     "start_time": "2023-11-26T00:47:27.553164Z"
    }
   },
   "id": "MKI0ObxP5mHe",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Avaliando a Distribuição da Coluna Alvo\n",
    "\n",
    "Prosseguindo com a exploração dos dados, focamos na coluna 'Target', que desempenha um papel crucial na nossa análise. Para entender melhor a distribuição dos valores nesta coluna, utilizamos o método `value_counts()` do Pandas. Este método nos retorna rapidamente um panorama da frequência de cada valor único na coluna 'Target'.\n",
    "\n",
    "Ao realizar esta operação, estamos buscando informações sobre a distribuição das classes ou categorias representadas na coluna 'Target'. Esta informação é vital, pois uma distribuição desigual das classes pode influenciar significativamente o desempenho do modelo.\n",
    "\n",
    "Assim, ao imprimir o resultado de `value_counts()`, estabelecemos uma base para compreender como os dados estão estruturados em relação à variável que pretendemos prever.\n",
    "\n",
    "Como esperado, há um forte desbalanceamento entre os valores."
   ],
   "metadata": {
    "id": "zHhMIDrD5pjj"
   },
   "id": "zHhMIDrD5pjj"
  },
  {
   "cell_type": "code",
   "source": [
    "print(df['Target'].value_counts())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sArFNUk65neM",
    "outputId": "eb4f744b-7a16-4254-8a8f-2c95224ff56c",
    "ExecuteTime": {
     "end_time": "2023-11-26T00:47:27.557257Z",
     "start_time": "2023-11-26T00:47:27.555503Z"
    }
   },
   "id": "sArFNUk65neM",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Preparação dos Dados para Modelagem\n",
    "\n",
    "Como estamos trabalhando com PyTorch, é boa prática criar uma classe customizada que extende a classe Dataset. Definimos um construtor com os parâmetros `X` e `y` a serem informados externamente. Esta abordagem foi escolhida pois facilita a normalização dos dados fora do contexto desta classe e simplifica sua reutilização.\n",
    "\n",
    "Os métodos `__len__` e `__getitem__` também foram implementados pois são necessários para trabalhar em conjunto com o `DataLoader`.\n",
    "\n",
    "Com o objetivo de adaptar nosso conjunto de dados para um problema de classificação binária, fizemos ajustes focando nas categorias 'Graduate' e 'Dropout'. Criamos a nova coluna 'Dropout' no DataFrame, codificando 'Graduate' como 0 e 'Dropout' como 1, que será nossa variável dependente. Eliminamos a categoria 'Enrolled', o que também contribui para reduzir o desbalanceamento inicial das classes.\n",
    "\n",
    "Com a nova organização dos dados, removemos a coluna 'Target', pois ela já foi codificada em 'Dropout'. Em seguida, separamos o DataFrame em variáveis independentes `X` e a variável dependente `y`, que agora reflete com precisão o nosso problema binário de prever se um aluno irá se formar ou desistir."
   ],
   "metadata": {
    "id": "72UZHs6i5uwU"
   },
   "id": "72UZHs6i5uwU"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "df = df[df['Target'].isin(['Graduate', 'Dropout'])]\n",
    "df['Dropout'] = [0 if typ == 'Graduate' else 1 for typ in df['Target']]\n",
    "df.drop(['Target'], axis=1, inplace=True)\n",
    "\n",
    "X = df.values[:, :-1].astype('float32')\n",
    "y = LabelEncoder().fit_transform(df.values[:, -1]).astype('float32').reshape((-1, 1))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6xtOI-mv4-8",
    "outputId": "291ee413-22cf-43bd-de60-cd140c80ffb8",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.557158Z"
    }
   },
   "id": "l6xtOI-mv4-8",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Separando os Dados em Treinamento e Teste\n",
    "\n",
    "Agora avançamos para  a divisão do conjunto de dados em partes para treinamento e teste. Ao definirmos o parâmetro `test_size` como 0.3, estamos especificando que 30% do conjunto de dados será reservado para teste e 70% para treinar o modelo, conforme solicitado.\n",
    "\n",
    "Após a aplicação da `train_test_split`, obtemos quatro conjuntos de dados: `X_train` e `y_train`, utilizados para treinamento, e `X_test` e `y_test`, que servirão para testar o modelo após o treinamento."
   ],
   "metadata": {
    "id": "B_8WrMIr512Q"
   },
   "id": "B_8WrMIr512Q"
  },
  {
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify=y)"
   ],
   "metadata": {
    "id": "yPkTGHlN51Xv",
    "ExecuteTime": {
     "end_time": "2023-11-26T00:47:27.567009Z",
     "start_time": "2023-11-26T00:47:27.558684Z"
    }
   },
   "id": "yPkTGHlN51Xv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Normalizando os Dados\n",
    "\n",
    "Iniciamos este processo utilizando o método `describe()` do DataFrame, que fornece um resumo estatístico, como média, desvio padrão, valores mínimos e máximos. Esta visão permite identificar a necessidade de normalização."
   ],
   "metadata": {
    "id": "XgfBQ4e657ot"
   },
   "id": "XgfBQ4e657ot"
  },
  {
   "cell_type": "code",
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "-KnwKhIH6Lda",
    "outputId": "d415f061-9868-4af8-e6ad-ec0c208abfe5",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.560268Z"
    }
   },
   "id": "-KnwKhIH6Lda",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Redução de Dimensionalidade e Pipeline de Pré-processamento\n",
    "\n",
    "Neste estágio, aprimoramos o pré-processamento de dados incorporando a Análise de Componentes Principais (PCA). Começamos inicialmente com a importação e configuração do PCA da biblioteca `sklearn.decomposition`, definindo-o para reter 80% da variância dos dados.\n",
    "\n",
    "No conjunto de treinamento (`X_train`), usamos o método `fit_transform` para ajustar o pipeline aos dados e, simultaneamente, transformá-los. O resultado dessa transformação é armazenado em `X_train_pca`. Para o conjunto de teste (`X_test`), empregamos o método `transform` para aplicar as mesmas transformações com os parâmetros ajustados durante o treinamento. Isso resulta em `X_test_pca`.\n",
    "\n",
    "Em seguida, criamos duas instâncias de `CSVDataset`, uma para o conjunto de treinamento e outra para o conjunto de testes."
   ],
   "metadata": {
    "id": "u8Sap9WA-cvQ"
   },
   "id": "u8Sap9WA-cvQ"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA(n_components=0.8)\n",
    "\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "X_train_pca = pipeline.fit_transform(X_train)\n",
    "\n",
    "X_test_pca = pipeline.transform(X_test)\n",
    "\n",
    "# Criar datasets PyTorch\n",
    "train_dataset = CSVDataset(X_train_pca, y_train)\n",
    "test_dataset = CSVDataset(X_test_pca, y_test)"
   ],
   "metadata": {
    "id": "MyO-XIZH57SV",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.561889Z"
    }
   },
   "id": "MyO-XIZH57SV",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset.X, test_dataset.X"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpPJaZT960a3",
    "outputId": "0919bafa-fda8-44d1-ee74-1738fb9d4db9",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.564351Z"
    }
   },
   "id": "dpPJaZT960a3",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aplicação do DataLoader\n",
    "\n",
    "A utilização de um `DataLoader` é boa prática para o treinamento de modelos de redes neurais com PyTorch. Ele encapsula o gerenciamento dos dados e permite customizações sob demanda.\n",
    "\n",
    "Definimos um `DataLoader` para o nosso conjunto de dados de testes. O tamanho do batch foi definido em 32. Decidimos por ativar o embaralhamento (`shuffle=True`) pois este pode ser benéfico ao treinamento, evitando que o modelo memorize a ordem dos dados. Se os dados não forem embaralhados, o modelo pode 'aprender' a predizer o próximo batch baseado na ordem dos dados, ao invés de fato aprender com os padrões presentes nestes dados [3].\n",
    "\n"
   ],
   "metadata": {
    "id": "uh9tu6f367CF"
   },
   "id": "uh9tu6f367CF"
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "id": "1JJm7MSLv9bb",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.566114Z"
    }
   },
   "id": "1JJm7MSLv9bb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Definindo o Modelo de Rede Neural\n",
    "\n",
    "Determinamos o número de características de entrada (`n_inputs`) com base na forma do conjunto de dados de teste processado pelo PCA. Esse número guia a configuração da primeira camada da rede.\n",
    "\n",
    "O modelo é construído como uma sequência de camadas, cada uma com a função de ativação `relu`, exceto a última camada que utiliza `sigmoid`. A primeira camada recebe `shape` neurônios, correspondendo ao número de características de entrada. A segunda camada tem metade do número de neurônios da primeira, seguindo uma estrutura que gradualmente condensa a informação. A última camada, com um único neurônio e ativação `sigmoid`, é adequada para uma tarefa de classificação binária, produzindo uma saída entre 0 e 1.\n",
    "\n",
    "A função de perda escolhida foi a `BCELoss` (Binary Cross Entropy Loss) e o optimizador `Adam`, ambos adequados para a proposta de classificação binária."
   ],
   "metadata": {
    "id": "BPmYCJp86-TB"
   },
   "id": "BPmYCJp86-TB"
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.optim import SGD, Adam\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.hidden1 = nn.Linear(n_inputs, 8)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.hidden2 = nn.Linear(8, 4)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.hidden3 = nn.Linear(4, 1)\n",
    "        self.act3 = nn.Sigmoid()\n",
    "\n",
    "    # forward\n",
    "    def forward(self, X):\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "model = MLP(train_dataset.X.shape[1]).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uTA0W8-gv_Fi",
    "outputId": "c5c662ba-15af-43f3-80b1-855db8ba4369",
    "ExecuteTime": {
     "end_time": "2023-11-26T00:47:27.570472Z",
     "start_time": "2023-11-26T00:47:27.567716Z"
    }
   },
   "id": "uTA0W8-gv_Fi",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Treinamento do Modelo\n",
    "\n",
    "Configuramos um total de 10 iterações no treinamento do modelo. Este valor foi definido empiricamente.\n",
    "\n",
    "Ao final do treinamento, o histórico é armazenado em uma variável acessória. Este histórico contém informações valiosas sobre a evolução da função de perda ao longo das épocas, permitindo a análise posterior do desempenho do modelo."
   ],
   "metadata": {
    "id": "KpdWpIR47EcH"
   },
   "id": "KpdWpIR47EcH"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "EPOCHS = 10\n",
    "loss_list = np.zeros((EPOCHS,))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_batch = 0\n",
    "\n",
    "    for x_batch, y_batch in train:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # compute the model output\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_batch = loss_batch + loss.item()\n",
    "\n",
    "    loss_list[epoch] = loss_batch / len(x_batch)"
   ],
   "metadata": {
    "id": "JI23XnGrwDob",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.568791Z"
    }
   },
   "id": "JI23XnGrwDob",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Curva de Aprendizado e Avaliação do Desempenho do Modelo\n",
    "\n",
    "Depois o treinamento, exibidos o histórico em um gráfico para análise.\n",
    "\n",
    "O gráfico apresenta a evolução da perda ao longo das épocas. Observamos que a perda no conjunto de validação diminui de forma consistente ao longo do tempo, o que indica que o modelo está aprendendo efetivamente e melhorando a sua capacidade de generalização."
   ],
   "metadata": {
    "id": "nWMWUAtbYg8k"
   },
   "id": "nWMWUAtbYg8k"
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(loss_list):\n",
    "\n",
    "  plt.plot(loss_list)\n",
    "  plt.ylabel(\"Train Loss\")\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.show()\n",
    "\n",
    "plot_history(loss_list)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "A6lqS7mmwEvT",
    "outputId": "96d6fbcb-8616-4cdb-c89e-fc7485f96173",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.569776Z"
    }
   },
   "id": "A6lqS7mmwEvT",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Avaliação Final do Modelo\n",
    "\n",
    "Concluído o treinamento da rede neural, procedemos à sua avaliação usando o conjunto de dados `test_dataset`. Note que o `DataLoader` está sendo utilizado e o parâmetro `batch_size=len(test_dataset)` indica que todo o conjunto de teste está sendo retornado para as variáveis `xx_test` e `yy_test`.\n",
    "\n",
    "Em seguida, invocamos o modelo, passando o conjunto de teste `xx_test`, não sem antes enviá-lo para o contexto correto de execução. Em seguida, o retorno da predição é arredondado ( `round()`) e armazenado na variável `yy_pred`.\n",
    "\n",
    "Por fim, calculamos a acurácia geral do modelo, que nos informa uma proporção de previsões corretas sobre o total de previsões. Com uma acurácia de aproximadamente 88%, podemos concluir que o modelo é bastante eficaz."
   ],
   "metadata": {
    "id": "p2tzzQMzYu_Y"
   },
   "id": "p2tzzQMzYu_Y"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "xx_test, yy_test = next(iter(DataLoader(test_dataset, batch_size=len(test_dataset))))\n",
    "\n",
    "yy_pred = model(xx_test.to(device)).round()\n",
    "\n",
    "print('Acuracidade: ', accuracy_score(yy_test.detach().cpu().numpy(), yy_pred.detach().cpu().numpy()))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfEdGCLfwX_a",
    "outputId": "5f44afe8-2c04-47aa-c072-022c2d1fcb5f",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.570714Z"
    }
   },
   "id": "cfEdGCLfwX_a",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adicionando a Regularização L2\n",
    "\n",
    "Apesar da boa eficácia do modelo, em torno de 88%, ainda é possível explorar outras técnicas para obter melhores resultados. Neste exemplo, utilizaremos a Regularização L2, disponibilizada pelo parâmetro `weight_decay` da função de otimização. O valor `0.001` foi definido empiricamente.\n",
    "\n",
    "Em seguida, treinamentos novamente o modelo com os mesmos parâmetros do treinamento anterior e avaliamos os resultados."
   ],
   "metadata": {
    "id": "irh_qKn4CfNw"
   },
   "id": "irh_qKn4CfNw"
  },
  {
   "cell_type": "code",
   "source": [
    "model = MLP(train_dataset.X.shape[1]).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# weight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\n",
    "optimizer = Adam(model.parameters(),  weight_decay=0.001, lr=0.001)\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ze96TeS1Ce3o",
    "outputId": "a9da8fc6-767d-42e8-f6f3-cd8e9689978d",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.571782Z"
    }
   },
   "id": "ze96TeS1Ce3o",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "EPOCHS = 10\n",
    "loss_list = np.zeros((EPOCHS,))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_batch = 0\n",
    "\n",
    "    for x_batch, y_batch in train:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # compute the model output\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_batch = loss_batch + loss.item()\n",
    "\n",
    "    loss_list[epoch] = loss_batch / len(x_batch)"
   ],
   "metadata": {
    "id": "r5YrA9NPDncJ",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.572996Z"
    }
   },
   "id": "r5YrA9NPDncJ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_history(loss_list)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "fhtHBB2lDqBq",
    "outputId": "46056596-4fe4-405b-ac53-f95985739238",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.573936Z"
    }
   },
   "id": "fhtHBB2lDqBq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "xx_test, yy_test = next(iter(test))\n",
    "yy_pred = model(xx_test.to(device)).round()"
   ],
   "metadata": {
    "id": "YbF-WPwPDtrE",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.575364Z"
    }
   },
   "id": "YbF-WPwPDtrE",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "A acurácia em torno de 88% demonstra que a técnica de Regularização L2 não foi eficaz em aprimorar a acuracidade do modelo."
   ],
   "metadata": {
    "id": "XO0hkLAucg9n"
   },
   "id": "XO0hkLAucg9n"
  },
  {
   "cell_type": "code",
   "source": [
    "print(accuracy_score(yy_test.detach().cpu().numpy(), yy_pred.detach().cpu().numpy()))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WTYoCAFMDvVl",
    "outputId": "eb9a0640-0d5f-450a-c24b-305c87f2a697",
    "ExecuteTime": {
     "start_time": "2023-11-26T00:47:27.576515Z"
    }
   },
   "id": "WTYoCAFMDvVl",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Conclusões\n",
    "\n",
    "Ao finalizar a análise e modelagem do conjunto de dados, concluímos que as redes neurais, com o uso do PyTorch, demonstraram ser ferramentas eficazes para prever a evasão e o sucesso acadêmico de estudantes.\n",
    "\n",
    "A aplicação de técnicas como normalização e PCA e contribuíram significativamente para a eficácia do modelo.\n",
    "\n",
    "A precisão alcançada, em torno de 88%, sugere que o modelo pode ser uma ferramenta valiosa para identificar estudantes em risco, permitindo intervenções proativas para melhorar a retenção e o sucesso acadêmico."
   ],
   "metadata": {
    "id": "0n_DklqKaaMJ"
   },
   "id": "0n_DklqKaaMJ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Referências\n",
    "\n",
    "1. University of California, Irvine. Predict Student's Dropout and Academic Success. UCI Machine Learning Repository. Disponível em: https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success. Acesso em: 23 nov.2023.\n",
    "\n",
    "2. Pierian Training. Machine Learning in Python: Principal Component Analysis (PCA). Disponível em: https://pieriantraining.com/machine-learning-in-python-principal-component-analysis-pca/. Acesso em: 23 nov.2023.\n",
    "\n",
    "3. Tantai, Hengtao. “Improving Control and Reproducibility of PyTorch DataLoader with Sampler Instead of Shuffle Argument.” Medium, 22 fev. 2023. Disponível em: https://medium.com/@zergtant/improving-control-and-reproducibility-of-pytorch-dataloader-with-sampler-instead-of-shuffle-7f795490256e. Acesso em: 25 nov. 2023.\n",
    "\n",
    "4. PyTorch. torch.optim.Adam. Disponível em: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html. Acesso em: 25 nov.2023."
   ],
   "metadata": {
    "id": "5ez4tqRYAeFd"
   },
   "id": "5ez4tqRYAeFd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
